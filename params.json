{"name":"Machine learning","tagline":"Machine Learning algorithms","body":"### Machine Learning sample algorithms\r\nThis page describes about my implementations of Machine learning algorithms. This specifically deals with supervised learning methods. The implementations are carried out mostly in matlab. The core notion is to given a set of data containing relations, the system should be able to learn from the data and able to apply the relations to the new data.\r\n\r\n### Terminologies\r\n## Vector\r\nSet of data that is generally mapped together in a structured format, probably in a matrix form which simplifies its operation.\r\n## Training\r\nThe process where the incoming training vectors consisting of the known labeling or classifications are fed to the system for data aggregation. The training vectors consist of the training vector and its corresponding label vector. This is the data from which the system learns and develops a mode for generalizing other data.\r\n## Testing\r\nThe process where the data that is to be classified is fed to the system. Testing set consists only the data whose classification is decided by the system.\r\n## Features and Weight\r\nThe input vector's classification may depend on different parameters. Those parameters are called as features. In a real-world scenario, not all features are given equal preferences which are otherwise known as weights. These shape the factor that classifies the vector.\r\n\r\n### Some pointers\r\nThe notion seems to be simple but care should be taken such that the training does not result in the classifier over-fitting the data. For example, for a large testing set, an ideal case would suggest that the system should learn the existing data and then apply it to the different data where the classes should also grow proportional to testing amount. In other words, the system should not generalize the test set to the pre-existing classifiers.\r\n\r\n## Projects\r\n### Naive Bayes classifier\r\nRefer [here](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) on how this works.\r\n#### What's special about it?\r\nTraining was carried out for random trials of 10%, 30% and 50% of the fisher-iris dataset. The bin\r\nwidth is also adjusted and the results are recorded. It can be observed that the less trained agent\r\nhave more misclassifications than the ones with a relatively greater training. Additionally, the bin\r\nwidth of 4 and 8 are tested and the inference is that the bin width with 8 has lesser number of\r\nmisclassifications than the other because the larger bin width spreads the incoming data\r\nuniformly.\r\n#### Project link\r\nImplementation on matlab. The project is [here](https://github.com/sudharsannr/MachineLearning/tree/master/NaiveBayes).\r\n\r\n### Contact author\r\nMaintained by @sudharsannr\r\nContact the author if it requires any edits.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}